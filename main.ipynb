{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries for fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to determine R home: [WinError 2] The system cannot find the file specified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import r\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch and convert cipher data to pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://nextcloud.sdu.dk/index.php/s/Zzjcqjopy5cTawn/download/data_33.Rdata\"\n",
    "data_file = \"data.Rdata\"\n",
    "\n",
    "#download the file\n",
    "req = requests.get(data_url, allow_redirects=True, stream=True)\n",
    "\n",
    "#save downloaded file\n",
    "with open(data_file,\"wb\") as rf:\n",
    "     for chunk in req.iter_content(chunk_size=1024):\n",
    "         # writing one chunk at a time to r file\n",
    "         if chunk:\n",
    "              rf.write(chunk)\n",
    "\n",
    "r_data=r.load(data_file)\n",
    "\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    ciphers = ro.conversion.rpy2py(r['ciphers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(arr, indexes):\n",
    "    mask = np.ones(len(arr), dtype=bool)\n",
    "    mask[indexes] = True\n",
    "    return arr[mask]\n",
    "\n",
    "def take_inverse(arr, indexes):\n",
    "    mask = np.ones(len(arr), dtype=bool)\n",
    "    mask[indexes] = False\n",
    "    return arr[mask]\n",
    "\n",
    "def all_persons_in(ciphers, split = 0.5):\n",
    "    count = len(ciphers)\n",
    "    samples = random.sample(range(0, count), int(count * split))\n",
    "    train_data = take(ciphers, samples)\n",
    "    test_data = take_inverse(ciphers, samples)\n",
    "    return { \"train_data\": train_data, \"test_data\": test_data }\n",
    "\n",
    "def disjunct(ciphers, amount):\n",
    "    total_people = len(set(ciphers.T[0]))\n",
    "    per_person = int(len(ciphers) / total_people)\n",
    "    train_data = take(ciphers, range(0, per_person * amount))\n",
    "    test_data = take_inverse(ciphers, range(0, per_person * amount))\n",
    "    return { \"train_data\": train_data, \"test_data\": test_data }\n",
    "\n",
    "def split(ciphers):\n",
    "    person_index = ciphers.T[0]\n",
    "    ground_truth = ciphers.T[1]\n",
    "    data = []\n",
    "    for cipher in ciphers:\n",
    "        data.append(cipher[2:len(cipher)])\n",
    "    return { \"person\": person_index, \"truth\":ground_truth, \"data\": data}\n",
    "\n",
    "def compute_accuracy_folds(folds, ciphers, predictor, predictor_args = {}):\n",
    "    results = []\n",
    "    for fold in folds:\n",
    "        train_raw = split(take(ciphers, fold))\n",
    "        test_raw = split(take_inverse(ciphers, fold))\n",
    "\n",
    "        train_data = train_raw['data']\n",
    "        train_labels = train_raw['truth']\n",
    "        test_data = test_raw['data']\n",
    "        test_labels = test_raw['truth']\n",
    "\n",
    "        predictions = predictor(train_data, train_labels, test_data, predictor_args)\n",
    "        cf = pd.crosstab(predictions, test_labels)\n",
    "        accuracy = np.diag(cf).sum() / cf.to_numpy().sum()\n",
    "\n",
    "        results.append(accuracy)\n",
    "\n",
    "    mean = statistics.mean(results)\n",
    "    stdev = statistics.stdev(results)\n",
    "    return { \"mean\": mean, \"stdev\": stdev }\n",
    "\n",
    "def k_folds(arr_size, folds):\n",
    "    samples = random.sample(range(0, arr_size), arr_size)\n",
    "    return np.array_split(samples, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA and other preprocessing on ciphers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def decompose(ciphers, cumsum):\n",
    "    cipher_length = len(ciphers.T)\n",
    "\n",
    "    metadata = np.delete(ciphers, range(2, cipher_length), axis=1)\n",
    "    copy = np.delete(ciphers, [0,1], axis=1)\n",
    "\n",
    "    pca = PCA(cumsum)\n",
    "    pca.fit(copy)\n",
    "    transformed = pca.transform(copy)\n",
    "\n",
    "    ciphers_pca = np.append(metadata, transformed, axis=1)\n",
    "    return ciphers_pca\n",
    "\n",
    "ciphers_pca = decompose(ciphers, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross-validated kNN analysis with and without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# kNN functions\n",
    "def knn_predictor(train_data, train_labels, test_data, args):\n",
    "    k = args['k']\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(train_data, train_labels)\n",
    "    return neigh.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 0.9340454545454545, 'stdev': 0.00043199307815894913}\n"
     ]
    }
   ],
   "source": [
    "# Without preprocessing\n",
    "data = all_persons_in(ciphers, 0.5)\n",
    "print(compute_accuracy_folds(k_folds(66000, 10), ciphers, knn_predictor, {'k': 3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 0.9579545454545455, 'stdev': 0.00020985703649062027}\n"
     ]
    }
   ],
   "source": [
    "# With preprocessing\n",
    "data = all_persons_in(ciphers_pca, 0.5)\n",
    "print(compute_accuracy_folds(k_folds(66000, 10), ciphers_pca, knn_predictor, {'k': 3}))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
